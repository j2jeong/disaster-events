import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import sys
import os
import pathlib
from typing import Dict, Any, List

def _parse_iso(dt: str) -> float:
    """ISO ë‚ ì§œ ë¬¸ìì—´ì„ timestampë¡œ ë³€í™˜"""
    try:
        if dt.endswith('Z'):
            dt = dt.replace('Z', '+00:00')
        elif not dt.endswith(('+00:00', '-')):
            dt += '+00:00'
        return datetime.fromisoformat(dt).timestamp()
    except Exception as e:
        print(f"Date parsing error for '{dt}': {e}")
        return 0.0

def clean_duplicate_key(title: str, date: str, lat: str, lon: str) -> str:
    """ì¤‘ë³µ ì œê±°ìš© í‚¤ ìƒì„± (ì •ê·œí™”)"""
    clean_title = re.sub(r'[^\w\s]', '', title.lower()).strip()
    clean_title = re.sub(r'\s+', ' ', clean_title)
    
    try:
        lat_clean = f"{float(lat):.4f}" if lat else "0"
        lon_clean = f"{float(lon):.4f}" if lon else "0"
    except:
        lat_clean = "0"
        lon_clean = "0"
    
    date_clean = date[:10] if len(date) >= 10 else date
    
    return f"{clean_title}|{date_clean}|{lat_clean}|{lon_clean}"


def merge_events(new_events: List[Dict[str, Any]], 
                 existing_path: str = "docs/data/events.json",
                 past_events_path: str = "docs/data/past_events.json") -> List[Dict[str, Any]]:
    """
    ê°œì„ ëœ ëˆ„ì  ë°ì´í„° ë³‘í•© ì‹œìŠ¤í…œ
    - GitHub Actions í™˜ê²½ì— ìµœì í™”
    - ì•ˆì •ì ì¸ ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ
    - ìƒì„¸í•œ ë¡œê¹… ë° ê²€ì¦
    """
    print("=" * 80)
    print("ğŸ”„ STARTING ENHANCED DATA MERGE PROCESS")
    print("=" * 80)
    
    merged: Dict[str, Dict[str, Any]] = {}
    stats = {
        'past_events_loaded': 0,
        'existing_events_loaded': 0,
        'new_events_provided': len(new_events),
        'new_events_added': 0,
        'events_updated': 0,
        'duplicates_removed': 0,
        'old_events_archived': 0,
        'validation_errors': 0
    }
    
    # íŒŒì¼ ê²½ë¡œ ê²€ì¦ ë° ìƒì„±
    existing_path_obj = pathlib.Path(existing_path)
    past_path_obj = pathlib.Path(past_events_path)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    existing_path_obj.parent.mkdir(parents=True, exist_ok=True)
    past_path_obj.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ Working directories:")
    print(f"  - Current events: {existing_path}")
    print(f"  - Past events: {past_events_path}")
    print(f"  - New events provided: {len(new_events)}")
    
    try:
        # 1. ê³¼ê±° ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ (ìµœê³  ìš°ì„ ìˆœìœ„ - ë³´ì¡´ë˜ì–´ì•¼ í•¨)
        print(f"\nğŸ“š Loading archived events from {past_events_path}...")
        if past_path_obj.exists() and past_path_obj.stat().st_size > 0:
            try:
                past_content = past_path_obj.read_text(encoding="utf-8")
                if past_content.strip():
                    past_events = json.loads(past_content)
                    print(f"âœ… Successfully loaded {len(past_events)} archived events")
                    
                    for ev in past_events:
                        event_id = str(ev.get("event_id", "")).strip()
                        if event_id and event_id != "":
                            # ê³¼ê±° ì´ë²¤íŠ¸ëŠ” ë¬´ì¡°ê±´ ë³´ì¡´
                            merged[event_id] = ev
                            stats['past_events_loaded'] += 1
                            # ì•„ì¹´ì´ë¸Œ í”Œë˜ê·¸ ì¶”ê°€
                            merged[event_id]['_archived'] = True
                else:
                    print(f"âš ï¸ {past_events_path} exists but is empty")
            except json.JSONDecodeError as e:
                print(f"âŒ JSON parsing error in past events: {e}")
                # ë°±ì—… ìƒì„±
                backup_path = past_path_obj.with_suffix('.json.backup')
                past_path_obj.rename(backup_path)
                print(f"ğŸ“¦ Corrupted file backed up to {backup_path}")
            except Exception as e:
                print(f"âš ï¸ Error loading past events: {e}")
        else:
            print(f"ğŸ“ No existing archived events file found")
        
        # 2. í˜„ì¬ ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“‚ Loading current events from {existing_path}...")
        if existing_path_obj.exists() and existing_path_obj.stat().st_size > 0:
            try:
                existing_content = existing_path_obj.read_text(encoding="utf-8")
                if existing_content.strip():
                    existing_events = json.loads(existing_content)
                    print(f"âœ… Successfully loaded {len(existing_events)} current events")
                    
                    for ev in existing_events:
                        event_id = str(ev.get("event_id", "")).strip()
                        if event_id and event_id != "":
                            existing_in_merged = merged.get(event_id)
                            
                            if existing_in_merged and existing_in_merged.get('_archived'):
                                # ì´ë¯¸ ì•„ì¹´ì´ë¸Œëœ ì´ë²¤íŠ¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                                print(f"  ğŸ“š Preserving archived event: {event_id}")
                                continue
                            elif existing_in_merged:
                                # ê¸°ì¡´ ê²ƒì´ ë” ìµœì‹ ì´ë©´ ìœ ì§€
                                existing_time = _parse_iso(ev.get("crawled_at", ""))
                                merged_time = _parse_iso(existing_in_merged.get("crawled_at", ""))
                                if existing_time > merged_time:
                                    merged[event_id] = ev
                            else:
                                merged[event_id] = ev
                                stats['existing_events_loaded'] += 1
                else:
                    print(f"âš ï¸ {existing_path} exists but is empty")
            except json.JSONDecodeError as e:
                print(f"âŒ JSON parsing error in current events: {e}")
                stats['validation_errors'] += 1
            except Exception as e:
                print(f"âš ï¸ Error loading current events: {e}")
        else:
            print(f"ğŸ“ No existing current events file found")
        
        # 3. ìƒˆë¡œìš´ ë°ì´í„° í†µí•©
        print(f"\nğŸ†• Processing {len(new_events)} new events...")
        
        for i, ev in enumerate(new_events, 1):
            if i % 50 == 0:
                print(f"  â³ Processing event {i}/{len(new_events)}...")
            
            # ë°ì´í„° ê²€ì¦
            if not isinstance(ev, dict):
                stats['validation_errors'] += 1
                continue
            
            event_id = str(ev.get("event_id", "")).strip()
            
            # event_idê°€ ì—†ëŠ” ê²½ìš° ì„ì‹œ ID ìƒì„±
            if not event_id or event_id == "":
                title = str(ev.get('event_title', '')).strip()[:50]
                date = str(ev.get('event_date_utc', '')).strip()[:10]
                lat = str(ev.get('latitude', '')).strip()[:10]
                lon = str(ev.get('longitude', '')).strip()[:10]
                
                if not title:  # ì œëª©ë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    stats['validation_errors'] += 1
                    continue
                
                event_id = f"TEMP_{hash(f'{title}_{date}_{lat}_{lon}')}"
                ev["event_id"] = event_id
            
            # ê¸°ì¡´ ì´ë²¤íŠ¸ í™•ì¸
            existing = merged.get(event_id)
            
            if existing and existing.get('_archived'):
                # ì•„ì¹´ì´ë¸Œëœ ì´ë²¤íŠ¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                continue
            elif existing:
                # ë” ìµœì‹  ê²ƒìœ¼ë¡œ ì—…ë°ì´íŠ¸
                new_time = _parse_iso(ev.get("crawled_at", ""))
                existing_time = _parse_iso(existing.get("crawled_at", ""))
                
                if new_time >= existing_time:
                    merged[event_id] = ev
                    stats['events_updated'] += 1
                    if i <= 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
                        print(f"  ğŸ”„ Updated: {event_id}")
            else:
                # ì™„ì „íˆ ìƒˆë¡œìš´ ì´ë²¤íŠ¸
                merged[event_id] = ev
                stats['new_events_added'] += 1
                if i <= 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
                    print(f"  âœ¨ New: {event_id}")
        
        print(f"âœ… ID-based merge completed: {len(merged)} unique events")
        
        # 4. ë³´ì¡° í‚¤ë¡œ ì¤‘ë³µ ì œê±° (ì„œë¡œ ë‹¤ë¥¸ IDì§€ë§Œ ì‹¤ì§ˆì ìœ¼ë¡œ ê°™ì€ ì´ë²¤íŠ¸)
        print(f"\nğŸ” Performing content-based deduplication...")
        seen_keys = set()
        deduped = []
        duplicate_count = 0
        
        # ì•„ì¹´ì´ë¸Œëœ ê²ƒë“¤ì„ ë¨¼ì € ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ ë³´ì¥)
        archived_events = [ev for ev in merged.values() if ev.get('_archived')]
        non_archived_events = [ev for ev in merged.values() if not ev.get('_archived')]
        
        for ev in archived_events + non_archived_events:
            # í•„ìˆ˜ ë°ì´í„° ê²€ì¦
            title = str(ev.get('event_title', '')).strip()
            if not title:
                stats['validation_errors'] += 1
                continue
            
            # ì¤‘ë³µ ê²€ì‚¬ í‚¤ ìƒì„±
            date = str(ev.get('event_date_utc', '')).strip()
            lat = str(ev.get('latitude', '')).strip()
            lon = str(ev.get('longitude', '')).strip()
            
            key = clean_duplicate_key(title, date, lat, lon)
            
            if key not in seen_keys:
                seen_keys.add(key)
                # _archived í”Œë˜ê·¸ ì œê±° (ì¶œë ¥ìš©)
                if '_archived' in ev:
                    ev_copy = ev.copy()
                    del ev_copy['_archived']
                    deduped.append(ev_copy)
                else:
                    deduped.append(ev)
            else:
                duplicate_count += 1
        
        stats['duplicates_removed'] = duplicate_count
        print(f"âœ… Content-based deduplication: removed {duplicate_count} duplicates")
        
        # 5. ì‹œê°„ë³„ ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ (30ì¼ ì´ìƒëœ ê²ƒë“¤ì„ past_events.jsonìœ¼ë¡œ ì´ë™)
        cutoff_date = datetime.now() - timedelta(days=30)
        recent_events = []
        old_events = []
        
        print(f"\nğŸ“… Separating events by age (cutoff: {cutoff_date.strftime('%Y-%m-%d')})...")
        
        for event in deduped:
            try:
                # crawled_at ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨
                crawl_time_str = event.get('crawled_at', '')
                if crawl_time_str:
                    crawl_time = datetime.fromisoformat(crawl_time_str.replace('Z', '+00:00'))
                    if crawl_time >= cutoff_date:
                        recent_events.append(event)
                    else:
                        old_events.append(event)
                else:
                    # crawled_atì´ ì—†ìœ¼ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
                    recent_events.append(event)
            except Exception as e:
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
                recent_events.append(event)
        
        stats['old_events_archived'] = len(old_events)
        print(f"  ğŸ“Š Recent events (keep in main): {len(recent_events)}")
        print(f"  ğŸ“š Old events (move to archive): {len(old_events)}")
        
        # 6. ê³¼ê±° ì´ë²¤íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸
        if old_events:
            print(f"\nğŸ“š Updating archived events file...")
            
            # ê¸°ì¡´ ì•„ì¹´ì´ë¸Œì™€ ë³‘í•©
            try:
                existing_past = []
                if past_path_obj.exists():
                    past_content = past_path_obj.read_text(encoding="utf-8")
                    if past_content.strip():
                        existing_past = json.loads(past_content)
                
                # ì•„ì¹´ì´ë¸Œ ë°ì´í„°ë„ ì¤‘ë³µ ì œê±°
                archive_merged = {}
                for ev in existing_past + old_events:
                    event_id = ev.get("event_id")
                    if event_id:
                        existing_archive = archive_merged.get(event_id)
                        if not existing_archive:
                            archive_merged[event_id] = ev
                        else:
                            # ë” ìµœì‹  ê²ƒìœ¼ë¡œ ìœ ì§€
                            new_time = _parse_iso(ev.get("crawled_at", ""))
                            existing_time = _parse_iso(existing_archive.get("crawled_at", ""))
                            if new_time > existing_time:
                                archive_merged[event_id] = ev
                
                final_archive = list(archive_merged.values())
                final_archive.sort(key=lambda x: _parse_iso(x.get("crawled_at", "")), reverse=True)
                
                # ì•„ì¹´ì´ë¸Œ íŒŒì¼ ì €ì¥
                with open(past_path_obj, 'w', encoding='utf-8') as f:
                    json.dump(final_archive, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… Updated archive with {len(final_archive)} total archived events")
                
            except Exception as e:
                print(f"âŒ Error updating archive: {e}")
                stats['validation_errors'] += 1
        
        # 7. ìµœì¢… ì •ë ¬ ë° ê²€ì¦
        final_events = recent_events
        final_events.sort(key=lambda x: _parse_iso(x.get("crawled_at", "")), reverse=True)
        
        # ìµœì¢… ê²€ì¦
        valid_events = []
        for event in final_events:
            if (event.get('event_title') and 
                event.get('event_id') and 
                event.get('event_category')):
                valid_events.append(event)
            else:
                stats['validation_errors'] += 1
        
        final_events = valid_events
        
        # 8. í†µê³„ ë° ê²°ê³¼ ì¶œë ¥
        print(f"\n" + "=" * 60)
        print(f"ğŸ“Š MERGE PROCESS COMPLETED")
        print(f"=" * 60)
        print(f"Past events loaded: {stats['past_events_loaded']}")
        print(f"Existing events loaded: {stats['existing_events_loaded']}")
        print(f"New events provided: {stats['new_events_provided']}")
        print(f"New events added: {stats['new_events_added']}")
        print(f"Events updated: {stats['events_updated']}")
        print(f"Content duplicates removed: {stats['duplicates_removed']}")
        print(f"Events archived: {stats['old_events_archived']}")
        print(f"Validation errors: {stats['validation_errors']}")
        print(f"Final events in main file: {len(final_events)}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        if final_events:
            category_stats = {}
            for event in final_events:
                cat = event.get('event_category', 'Unknown')
                category_stats[cat] = category_stats.get(cat, 0) + 1
            
            print(f"\nğŸ“ˆ Categories in final dataset:")
            for cat, count in sorted(category_stats.items()):
                print(f"  {cat}: {count}")
        
        print(f"=" * 60)
        
        return final_events
        
    except Exception as e:
        print(f"\nğŸ’¥ CRITICAL ERROR in merge process: {e}")
        import traceback
        traceback.print_exc()
        
        # ë¹„ìƒ ë³µêµ¬: ìµœì†Œí•œ ìƒˆ ë°ì´í„°ë¼ë„ ë°˜í™˜
        print(f"ğŸ”„ Emergency fallback: returning new events only")
        return new_events if new_events else []

def create_backup_if_needed(events_path: str = "docs/data/events.json"):
    """
    í¬ê´„ì ì¸ ë°±ì—… ì‹œìŠ¤í…œ (GitHub Actionsìš©)
    """
    try:
        events_file = pathlib.Path(events_path)
        if not events_file.exists() or events_file.stat().st_size == 0:
            print("âš ï¸ No data to backup")
            return
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        backup_dir = pathlib.Path("docs/data/backups")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë°±ì—…
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = backup_dir / f"events_backup_{timestamp}.json"
        
        # ë°±ì—… ìƒì„±
        backup_path.write_text(events_file.read_text(encoding="utf-8"), encoding="utf-8")
        print(f"âœ… Created timestamped backup: {backup_path}")
        
        # GitHub Actions run number ë°±ì—… (í™˜ê²½ë³€ìˆ˜ ìˆì„ ë•Œ)
        run_number = os.environ.get('GITHUB_RUN_NUMBER')
        if run_number:
            run_backup_path = backup_dir / f"events_run_{run_number}.json"
            backup_path.link_to(run_backup_path.resolve())  # í•˜ë“œë§í¬ ìƒì„±
            print(f"âœ… Created run-based backup: {run_backup_path}")
        
        # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ (ìµœê·¼ 10ê°œë§Œ ìœ ì§€)
        backups = sorted(backup_dir.glob("events_backup_*.json"))
        if len(backups) > 10:
            for old_backup in backups[:-10]:
                old_backup.unlink()
                print(f"ğŸ—‘ï¸ Removed old backup: {old_backup.name}")
        
    except Exception as e:
        print(f"âš ï¸ Error creating backup: {e}")

def validate_data_integrity(events: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹œìŠ¤í…œ
    """
    report = {
        'total_events': len(events),
        'valid_events': 0,
        'invalid_events': 0,
        'issues': [],
        'categories': {},
        'date_range': {},
        'coordinate_issues': 0,
        'missing_fields': {}
    }
    
    required_fields = ['event_id', 'event_title', 'event_category']
    
    for i, event in enumerate(events):
        is_valid = True
        event_issues = []
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
        for field in required_fields:
            if not event.get(field):
                is_valid = False
                event_issues.append(f"Missing {field}")
                report['missing_fields'][field] = report['missing_fields'].get(field, 0) + 1
        
        # ì¢Œí‘œ ê²€ì¦
        try:
            lat = float(event.get('latitude', 0))
            lon = float(event.get('longitude', 0))
            if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):
                is_valid = False
                event_issues.append("Invalid coordinates")
                report['coordinate_issues'] += 1
        except (ValueError, TypeError):
            is_valid = False
            event_issues.append("Invalid coordinate format")
            report['coordinate_issues'] += 1
        
        # ì¹´í…Œê³ ë¦¬ í†µê³„
        category = event.get('event_category', 'Unknown')
        report['categories'][category] = report['categories'].get(category, 0) + 1
        
        if is_valid:
            report['valid_events'] += 1
        else:
            report['invalid_events'] += 1
            report['issues'].append({
                'index': i,
                'event_id': event.get('event_id', 'Unknown'),
                'issues': event_issues
            })
    
    # ë‚ ì§œ ë²”ìœ„ ë¶„ì„
    valid_dates = []
    for event in events:
        try:
            if event.get('event_date_utc'):
                date = datetime.fromisoformat(event['event_date_utc'].replace('Z', '+00:00'))
                valid_dates.append(date)
        except:
            pass
    
    if valid_dates:
        report['date_range'] = {
            'earliest': min(valid_dates).isoformat(),
            'latest': max(valid_dates).isoformat(),
            'span_days': (max(valid_dates) - min(valid_dates)).days
        }
    
    return report

def update_past_events_archive():
    """
    events.jsonì˜ 30ì¼ ì´ìƒ ëœ ì´ë²¤íŠ¸ë“¤ì„ past_events.jsonìœ¼ë¡œ ì´ë™
    (ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ)
    """
    try:
        events_path = pathlib.Path("docs/data/events.json")
        past_path = pathlib.Path("docs/data/past_events.json")
        
        if not events_path.exists():
            return
        
        # í˜„ì¬ ì´ë²¤íŠ¸ë“¤ ë¡œë“œ
        with open(events_path, 'r', encoding='utf-8') as f:
            current_events = json.load(f)
        
        # ê¸°ì¡´ ê³¼ê±° ì´ë²¤íŠ¸ë“¤ ë¡œë“œ
        past_events = []
        if past_path.exists():
            try:
                with open(past_path, 'r', encoding='utf-8') as f:
                    past_events = json.load(f)
            except:
                pass
        
        # 30ì¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        cutoff_date = datetime.now() - timedelta(days=30)
        recent_events = []
        old_events = []
        
        for event in current_events:
            crawled_at = event.get('crawled_at', '')
            try:
                if crawled_at:
                    crawl_time = datetime.fromisoformat(crawled_at.replace('Z', '+00:00'))
                    if crawl_time >= cutoff_date:
                        recent_events.append(event)
                    else:
                        old_events.append(event)
                else:
                    recent_events.append(event)  # ë‚ ì§œ ì—†ìœ¼ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
            except:
                recent_events.append(event)  # íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
        
        if old_events:
            print(f"âœ“ Archiving {len(old_events)} old events to past_events.json")
            
            # ê¸°ì¡´ ê³¼ê±° ì´ë²¤íŠ¸ë“¤ê³¼ í•©ì¹˜ê¸°
            all_past_events = past_events + old_events
            
            # past_events.jsonì—ì„œ ì¤‘ë³µ ì œê±°
            past_merged = {}
            for ev in all_past_events:
                event_id = str(ev.get("event_id", "")).strip()
                if event_id:
                    # ë” ìµœì‹  ê²ƒìœ¼ë¡œ ìœ ì§€
                    if event_id in past_merged:
                        existing_time = _parse_iso(past_merged[event_id].get("crawled_at", ""))
                        new_time = _parse_iso(ev.get("crawled_at", ""))
                        if new_time > existing_time:
                            past_merged[event_id] = ev
                    else:
                        past_merged[event_id] = ev
            
            # past_events.json ì—…ë°ì´íŠ¸
            final_past_events = list(past_merged.values())
            final_past_events.sort(key=lambda x: _parse_iso(x.get("crawled_at", "")), reverse=True)
            
            with open(past_path, 'w', encoding='utf-8') as f:
                json.dump(final_past_events, f, ensure_ascii=False, indent=2)
            
            # events.jsonì„ ìµœì‹  ì´ë²¤íŠ¸ë“¤ë§Œìœ¼ë¡œ ì—…ë°ì´íŠ¸
            with open(events_path, 'w', encoding='utf-8') as f:
                json.dump(recent_events, f, ensure_ascii=False, indent=2)
            
            print(f"âœ“ Updated events.json with {len(recent_events)} recent events")
            print(f"âœ“ Updated past_events.json with {len(final_past_events)} archived events")
        
    except Exception as e:
        print(f"âš ï¸ Error updating past events archive: {e}")

class RSOECrawler:
    def __init__(self):
        self.base_url = "https://rsoe-edis.org"
        self.event_list_url = "https://rsoe-edis.org/eventList"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # í•„í„°ë§í•  ì¹´í…Œê³ ë¦¬ë“¤
        self.target_categories = {
            "War": "War",
            "Environment pollution": "Environment pollution", 
            "Industrial explosion": "Industrial explosion",
            "Surroundings explosion": "Surroundings explosion",
            "Fire in built environment": "Fire in built environment",
            "Earthquake": "Earthquake",
            "Landslide": "Landslide", 
            "Volcanic eruption": "Volcanic eruption",
            "Flood": "Flood"
        }
        
        self.collected_events = []
        
    def get_page_content(self, url, retries=3):
        """í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except requests.RequestException as e:
                print(f"âš ï¸ Request failed (attempt {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"âœ— Failed to fetch {url} after {retries} attempts")
                    return None
    
    def extract_all_event_links(self, html_content):
        """ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ì´ë²¤íŠ¸ ë§í¬ë“¤ ì¶”ì¶œ"""
        if not html_content:
            return []
            
        soup = BeautifulSoup(html_content, 'html.parser')
        event_links = []
        
        links = soup.find_all('a', href=re.compile(r'/eventList/details/\d+'))
        
        for link in links:
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                event_links.append(full_url)
        
        print(f"âœ“ Found {len(event_links)} event links on this page")
        return list(set(event_links))
    
    def find_pagination_links(self, html_content):
        """í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ë“¤ ì°¾ê¸°"""
        if not html_content:
            return []
            
        soup = BeautifulSoup(html_content, 'html.parser')
        page_urls = []
        
        pagination_links = soup.find_all('a', href=re.compile(r'page=\d+'))
        
        for link in pagination_links:
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                page_urls.append(full_url)
        
        return list(set(page_urls))
    
    def extract_event_details(self, event_url):
        """ê°œë³„ ì´ë²¤íŠ¸ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            html_content = self.get_page_content(event_url)
            if not html_content:
                return None
                
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì´ë²¤íŠ¸ ID ì¶”ì¶œ
            event_id_match = re.search(r'/details/(\d+)', event_url)
            event_id = event_id_match.group(1) if event_id_match else ""
            
            fields = {}
            
            # dt/dd íƒœê·¸ ìŒìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
            dt_elements = soup.find_all('dt')
            for dt in dt_elements:
                field_name = dt.get_text(strip=True)
                dd = dt.find_next_sibling('dd')
                if dd:
                    field_value = dd.get_text(strip=True)
                    fields[field_name] = field_value
            
            # í…Œì´ë¸” êµ¬ì¡°ì—ì„œ ì •ë³´ ì¶”ì¶œ
            if not fields:
                rows = soup.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        field_name = cells[0].get_text(strip=True)
                        field_value = cells[1].get_text(strip=True)
                        if field_name and field_value:
                            fields[field_name] = field_value
            
            # í…ìŠ¤íŠ¸ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì¶”ì¶œ
            if not fields.get("Event title") or not fields.get("Event category"):
                all_text = soup.get_text()
                lines = [line.strip() for line in all_text.split('\n') if line.strip()]
                
                for i, line in enumerate(lines):
                    if 'Event title' in line and i + 1 < len(lines):
                        fields["Event title"] = lines[i + 1]
                    elif 'Event category' in line and i + 1 < len(lines):
                        fields["Event category"] = lines[i + 1]
                    elif 'Event date (UTC)' in line and i + 1 < len(lines):
                        fields["Event date (UTC)"] = lines[i + 1]
                    elif 'Last update (UTC)' in line and i + 1 < len(lines):
                        fields["Last update (UTC)"] = lines[i + 1]
                    elif 'Latitude' in line and i + 1 < len(lines):
                        fields["Latitude"] = lines[i + 1]
                    elif 'Longitude' in line and i + 1 < len(lines):
                        fields["Longitude"] = lines[i + 1]
                    elif 'Area range' in line and i + 1 < len(lines):
                        fields["Area range"] = lines[i + 1]
                    elif 'Address/Affected area(s)' in line and i + 1 < len(lines):
                        fields["Address/Affected area(s)"] = lines[i + 1]
            
            title = fields.get("Event title", "").strip()
            category = fields.get("Event category", "").strip()
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë° í•„í„°ë§
            mapped_category = ""
            for target_cat, mapped_cat in self.target_categories.items():
                if target_cat.lower() in category.lower():
                    mapped_category = mapped_cat
                    break
            
            if not mapped_category:
                return None
            
            # ì†ŒìŠ¤ ë§í¬ ì¶”ì¶œ
            source_link = None
            source_links = soup.find_all('a', href=True)
            for link in source_links:
                href = link['href']
                if href.startswith('http') and 'rsoe-edis.org' not in href:
                    link_text = link.get_text(strip=True).lower()
                    parent_text = ""
                    if link.parent:
                        parent_text = link.parent.get_text().lower()
                    
                    if 'source' in link_text or 'source' in parent_text:
                        source_link = href
                        break
            
            if not source_link:
                for link in source_links:
                    href = link['href']
                    if href.startswith('http') and 'rsoe-edis.org' not in href:
                        source_link = href
                        break
            
            # ì¢Œí‘œ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ë¦¬
            latitude = fields.get("Latitude", "").strip()
            longitude = fields.get("Longitude", "").strip()
            
            try:
                if latitude:
                    lat_clean = re.sub(r'[Â°NSEW]', '', latitude).strip()
                    lat_float = float(lat_clean)
                    if not (-90 <= lat_float <= 90):
                        latitude = ""
                    else:
                        latitude = str(lat_float)
                if longitude:
                    lon_clean = re.sub(r'[Â°NSEW]', '', longitude).strip()
                    lon_float = float(lon_clean)
                    if not (-180 <= lon_float <= 180):
                        longitude = ""
                    else:
                        longitude = str(lon_float)
            except ValueError:
                latitude = ""
                longitude = ""
            
            event_data = {
                "event_id": event_id,
                "event_title": title,
                "event_category": mapped_category,
                "original_category": category,
                "source": source_link or "",
                "event_date_utc": fields.get("Event date (UTC)", "").strip(),
                "last_update_utc": fields.get("Last update (UTC)", "").strip(),
                "latitude": latitude,
                "longitude": longitude,
                "area_range": fields.get("Area range", "").strip(),
                "address": fields.get("Address/Affected area(s)", "").strip(),
                "crawled_at": datetime.now().isoformat(),
                "event_url": event_url
            }
            
            return event_data
            
        except Exception as e:
            print(f"âš ï¸ Error extracting details from {event_url}: {e}")
            return None
    
    def crawl_events(self):
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        print("=== STARTING RSOE EDIS EVENT CRAWLING ===")
        print(f"Target categories: {list(self.target_categories.keys())}")
        print("=" * 60)
        
        try:
            # ë©”ì¸ í˜ì´ì§€ ë¡œë“œ
            print("âœ“ Loading main event list page...")
            main_html = self.get_page_content(self.event_list_url)
            if not main_html:
                print("âœ— Failed to load main page")
                return False
            
            # ëª¨ë“  ì´ë²¤íŠ¸ ë§í¬ ì¶”ì¶œ
            all_event_links = self.extract_all_event_links(main_html)
            
            # ì¶”ê°€ í˜ì´ì§€ë“¤ë„ í™•ì¸ (ìµœëŒ€ 3í˜ì´ì§€ê¹Œì§€ë§Œ)
            print("âœ“ Looking for additional pages...")
            pagination_links = self.find_pagination_links(main_html)
            
            if pagination_links:
                print(f"âœ“ Found {len(pagination_links)} pagination links")
                max_pages = min(len(pagination_links), 3)  # GitHub Actions ì‹œê°„ ì œí•œ ê³ ë ¤
                
                for i, page_url in enumerate(pagination_links[:max_pages]):
                    try:
                        print(f"  â†’ Loading additional page {i+1}/{max_pages}...")
                        page_html = self.get_page_content(page_url)
                        if page_html:
                            page_links = self.extract_all_event_links(page_html)
                            all_event_links.extend(page_links)
                        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    except Exception as e:
                        print(f"âš ï¸ Error loading page {page_url}: {e}")
            
            all_event_links = list(set(all_event_links))
            print(f"âœ“ Total unique event links collected: {len(all_event_links)}")
            
            if not all_event_links:
                print("âœ— No event links found!")
                return False
            
            print("=" * 60)
            print("âœ“ Processing event detail pages...")
            print("=" * 60)
            
            # ê° ì´ë²¤íŠ¸ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            target_events_found = 0
            max_events_to_process = min(len(all_event_links), 100)  # 10ë¶„ ì£¼ê¸°ì´ë¯€ë¡œ ì œí•œ
            
            for i, event_url in enumerate(all_event_links[:max_events_to_process], 1):
                if i % 10 == 0:
                    print(f"[{i:3d}/{max_events_to_process}] Progress: {i/max_events_to_process*100:.1f}%")
                
                event_data = self.extract_event_details(event_url)
                if event_data:
                    self.collected_events.append(event_data)
                    target_events_found += 1
                    if i <= 5:  # ì²˜ìŒ 5ê°œë§Œ ìƒì„¸ ë¡œê·¸
                        print(f"  âœ“ [{i:3d}] COLLECTED: {event_data['event_id']} - {event_data['event_title'][:50]}...")
                
                # ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ë‹¹í•œ ë”œë ˆì´
                time.sleep(0.3)
            
            print("\n" + "=" * 60)
            print(f"âœ“ CRAWLING COMPLETED!")
            print(f"Total processed: {max_events_to_process} events")
            print(f"Target events collected: {len(self.collected_events)} events")
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            if self.collected_events:
                category_stats = {}
                for event in self.collected_events:
                    cat = event['event_category']
                    category_stats[cat] = category_stats.get(cat, 0) + 1
                
                print("\nNew events by category:")
                for cat, count in sorted(category_stats.items()):
                    print(f"  {cat}: {count}")
            
            print("=" * 60)
            return True
            
        except Exception as e:
            print(f"âœ— Error during crawling: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def save_to_json(self, filename="rsoe_events.json"):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.collected_events, f, ensure_ascii=False, indent=2)
            print(f"âœ“ Data saved to {filename}")
            return True
        except Exception as e:
            print(f"âœ— Error saving to JSON: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸŒ RSOE DISASTER DATA CRAWLER WITH CUMULATIVE MERGE")
    print("=" * 80)
    
    try:
        # ë°±ì—… ìƒì„±
        print("1. Creating backup of existing data...")
        create_backup_if_needed("docs/data/events.json")
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ë° ì‹¤í–‰
        print("\n2. Initializing crawler...")
        crawler = RSOECrawler()
        
        print("\n3. Starting crawling process...")
        success = crawler.crawl_events()
        
        if success and crawler.collected_events:
            print(f"\n4. Successfully collected {len(crawler.collected_events)} new events")
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì¶œë ¥
            category_counts = {}
            for event in crawler.collected_events:
                category = event['event_category']
                category_counts[category] = category_counts.get(category, 0) + 1
            
            print("\n=== NEW COLLECTION SUMMARY ===")
            print(f"Total new events: {len(crawler.collected_events)}")
            print("New events by category:")
            for category, count in sorted(category_counts.items()):
                print(f"  ğŸ“Š {category}: {count}")
            
        else:
            print(f"\n4. No new events collected (success={success})")
            crawler.collected_events = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ìƒˆ ì´ë²¤íŠ¸ê°€ ì—†ì–´ë„ ì‹¤í–‰)
        print(f"\n5. Merging with existing data...")
        merged = merge_events(
            crawler.collected_events, 
            existing_path="docs/data/events.json",
            past_events_path="docs/data/past_events.json"
        )
        
        # ë³‘í•©ëœ ê²°ê³¼ ì €ì¥
        print(f"\n6. Saving merged results...")
        output_file = "docs/data/events.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ Final merged events saved to {output_file}: {len(merged)} total events")
        
        # ê³¼ê±° ì´ë²¤íŠ¸ ì•„ì¹´ì´ë¹™ (ì„ íƒì  - 30ì¼ ì´ìƒëœ ê²ƒë“¤ì€ past_events.jsonìœ¼ë¡œ ì´ë™)
        print(f"\n7. Updating past events archive...")
        update_past_events_archive()
        
        # ìµœì¢… í†µê³„
        if merged:
            final_categories = {}
            latest_events = 0
            cutoff = datetime.now() - timedelta(days=7)
            
            for event in merged:
                cat = event.get('event_category', 'Unknown')
                final_categories[cat] = final_categories.get(cat, 0) + 1
                
                crawl_time = event.get('crawled_at', '')
                if crawl_time:
                    try:
                        crawl_dt = datetime.fromisoformat(crawl_time.replace('Z', '+00:00'))
                        if crawl_dt >= cutoff:
                            latest_events += 1
                    except:
                        pass
            
            print(f"\n=== FINAL DATABASE SUMMARY ===")
            print(f"ğŸ“ˆ Total events in database: {len(merged)}")
            print(f"ğŸ†• Events from last 7 days: {latest_events}")
            print(f"ğŸ“Š Categories in database:")
            for category, count in sorted(final_categories.items()):
                print(f"  {category}: {count}")
        
        print(f"\nâœ… PROCESS COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        return 0
        
    except Exception as e:
        print(f"\nâŒ FATAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        print("=" * 80)
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)